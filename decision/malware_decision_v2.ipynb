{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b715b0f7",
   "metadata": {},
   "source": [
    "# Training NN for the final Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b679ce-b29a-4b1c-b772-08fcab7c75c9",
   "metadata": {},
   "source": [
    "## Load Tensorflow and check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a3b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import sys\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1258a0e",
   "metadata": {},
   "source": [
    "# Load input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "069058d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 100000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dns_available</th>\n",
       "      <th>dns_nonzero</th>\n",
       "      <th>tls_available</th>\n",
       "      <th>tls_nonzero</th>\n",
       "      <th>ip_available</th>\n",
       "      <th>ip_nonzero</th>\n",
       "      <th>rdap_available</th>\n",
       "      <th>rdap_nonzero</th>\n",
       "      <th>geo_available</th>\n",
       "      <th>geo_nonzero</th>\n",
       "      <th>phishing_cnn_result</th>\n",
       "      <th>phishing_lgbm_result</th>\n",
       "      <th>phishing_xgboost_result</th>\n",
       "      <th>phishing_deepnn_result</th>\n",
       "      <th>phishing_dns_nn_result</th>\n",
       "      <th>phishing_rdap_nn_result</th>\n",
       "      <th>malware_cnn_result</th>\n",
       "      <th>malware_lgbm_result</th>\n",
       "      <th>malware_xgboost_result</th>\n",
       "      <th>dga_binary_nn_result</th>\n",
       "      <th>phishing_sum</th>\n",
       "      <th>phishing_avg</th>\n",
       "      <th>phishing_prod</th>\n",
       "      <th>malware_sum</th>\n",
       "      <th>malware_avg</th>\n",
       "      <th>malware_prod</th>\n",
       "      <th>total_sum</th>\n",
       "      <th>total_avg</th>\n",
       "      <th>total_prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54451</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008122</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>1.366864e-03</td>\n",
       "      <td>0.066331</td>\n",
       "      <td>0.013630</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.002436</td>\n",
       "      <td>6.067854e-23</td>\n",
       "      <td>1.092264</td>\n",
       "      <td>0.182044</td>\n",
       "      <td>2.825370e-11</td>\n",
       "      <td>1.002627</td>\n",
       "      <td>0.334209</td>\n",
       "      <td>4.655318e-07</td>\n",
       "      <td>2.094891</td>\n",
       "      <td>0.209489</td>\n",
       "      <td>7.981044e-40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247717</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>3.579364e-04</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>3.490693e-03</td>\n",
       "      <td>0.011647</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.017423</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15621</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.794902</td>\n",
       "      <td>0.967936</td>\n",
       "      <td>9.999437e-01</td>\n",
       "      <td>0.689681</td>\n",
       "      <td>0.941273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>7.239705e-07</td>\n",
       "      <td>5.393736</td>\n",
       "      <td>0.898956</td>\n",
       "      <td>4.994589e-01</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.396678</td>\n",
       "      <td>0.539668</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255533</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>3.409265e-07</td>\n",
       "      <td>0.055679</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>1.192370e-35</td>\n",
       "      <td>0.057303</td>\n",
       "      <td>0.009551</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.002644</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.059947</td>\n",
       "      <td>0.005995</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89645</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>6.330108e-04</td>\n",
       "      <td>0.071273</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>5.018924e-04</td>\n",
       "      <td>0.107293</td>\n",
       "      <td>0.017882</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.110089</td>\n",
       "      <td>0.011009</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343814</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>1.249553e-04</td>\n",
       "      <td>0.003323</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>3.854592e-02</td>\n",
       "      <td>1.005146</td>\n",
       "      <td>0.167524</td>\n",
       "      <td>6.602678e-17</td>\n",
       "      <td>1.002286</td>\n",
       "      <td>0.334095</td>\n",
       "      <td>1.171542e-07</td>\n",
       "      <td>2.045977</td>\n",
       "      <td>0.204598</td>\n",
       "      <td>2.981647e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104123</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>2.489036e-04</td>\n",
       "      <td>0.144647</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>8.776547e-26</td>\n",
       "      <td>0.157121</td>\n",
       "      <td>0.026187</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.002669</td>\n",
       "      <td>0.334223</td>\n",
       "      <td>7.199457e-07</td>\n",
       "      <td>1.159789</td>\n",
       "      <td>0.115979</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380872</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>1.054552e-02</td>\n",
       "      <td>0.051278</td>\n",
       "      <td>0.027170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.677820</td>\n",
       "      <td>0.568785</td>\n",
       "      <td>2.559731e-02</td>\n",
       "      <td>0.105889</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.246605</td>\n",
       "      <td>0.415535</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.378092</td>\n",
       "      <td>0.137809</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275135</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>2.749782e-03</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>7.275866e-01</td>\n",
       "      <td>0.015347</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.745217</td>\n",
       "      <td>0.074522</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232502</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>3.551183e-03</td>\n",
       "      <td>0.007138</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>9.142273e-07</td>\n",
       "      <td>1.020225</td>\n",
       "      <td>0.170037</td>\n",
       "      <td>4.292383e-13</td>\n",
       "      <td>1.002707</td>\n",
       "      <td>0.334236</td>\n",
       "      <td>8.102686e-07</td>\n",
       "      <td>2.022932</td>\n",
       "      <td>0.202293</td>\n",
       "      <td>3.179667e-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dns_available  dns_nonzero  tls_available  tls_nonzero  ip_available  \\\n",
       "54451             0.6        0.225       1.000000     0.833333           1.0   \n",
       "247717            1.0        0.750       0.833333     0.291667           1.0   \n",
       "15621             0.6        0.225       1.000000     0.833333           1.0   \n",
       "255533            0.9        0.525       0.833333     0.291667           1.0   \n",
       "89645             0.9        0.550       0.833333     0.291667           1.0   \n",
       "...               ...          ...            ...          ...           ...   \n",
       "343814            1.0        0.700       0.833333     0.291667           1.0   \n",
       "104123            0.6        0.200       1.000000     0.833333           1.0   \n",
       "380872            1.0        0.725       0.833333     0.291667           1.0   \n",
       "275135            1.0        0.700       0.833333     0.291667           1.0   \n",
       "232502            0.6        0.250       1.000000     0.708333           1.0   \n",
       "\n",
       "        ip_nonzero  rdap_available  rdap_nonzero  geo_available  geo_nonzero  \\\n",
       "54451        0.750        1.000000      0.833333            1.0     0.777778   \n",
       "247717       1.000        1.000000      0.708333            1.0     1.000000   \n",
       "15621        0.375        1.000000      0.708333            1.0     0.722222   \n",
       "255533       0.875        0.833333      0.250000            1.0     0.722222   \n",
       "89645        1.000        1.000000      0.708333            1.0     1.000000   \n",
       "...            ...             ...           ...            ...          ...   \n",
       "343814       1.000        0.833333      0.416667            1.0     1.000000   \n",
       "104123       0.500        0.833333      0.041667            1.0     0.666667   \n",
       "380872       0.625        1.000000      0.708333            1.0     0.722222   \n",
       "275135       1.000        1.000000      0.708333            1.0     1.000000   \n",
       "232502       0.500        0.875000      0.208333            1.0     0.666667   \n",
       "\n",
       "        phishing_cnn_result  phishing_lgbm_result  phishing_xgboost_result  \\\n",
       "54451                   1.0              0.008122                 0.002815   \n",
       "247717                  0.0              0.000682                 0.000621   \n",
       "15621                   1.0              0.794902                 0.967936   \n",
       "255533                  0.0              0.000314                 0.000381   \n",
       "89645                   0.0              0.000668                 0.000933   \n",
       "...                     ...                   ...                      ...   \n",
       "343814                  1.0              0.000353                 0.000631   \n",
       "104123                  0.0              0.000520                 0.000777   \n",
       "380872                  0.0              0.006750                 0.010146   \n",
       "275135                  0.0              0.000425                 0.000540   \n",
       "232502                  1.0              0.000999                 0.003145   \n",
       "\n",
       "        phishing_deepnn_result  phishing_dns_nn_result  \\\n",
       "54451             1.366864e-03                0.066331   \n",
       "247717            3.579364e-04                0.007201   \n",
       "15621             9.999437e-01                0.689681   \n",
       "255533            3.409265e-07                0.055679   \n",
       "89645             6.330108e-04                0.071273   \n",
       "...                        ...                     ...   \n",
       "343814            1.249553e-04                0.003323   \n",
       "104123            2.489036e-04                0.144647   \n",
       "380872            1.054552e-02                0.051278   \n",
       "275135            2.749782e-03                0.008847   \n",
       "232502            3.551183e-03                0.007138   \n",
       "\n",
       "        phishing_rdap_nn_result  malware_cnn_result  malware_lgbm_result  \\\n",
       "54451                  0.013630                 1.0             0.000191   \n",
       "247717                 0.002786                 0.0             0.000052   \n",
       "15621                  0.941273                 0.0             0.000533   \n",
       "255533                 0.000929                 0.0             0.000280   \n",
       "89645                  0.033787                 0.0             0.000061   \n",
       "...                         ...                 ...                  ...   \n",
       "343814                 0.000713                 1.0             0.000052   \n",
       "104123                 0.010928                 1.0             0.000305   \n",
       "380872                 0.027170                 0.0             0.677820   \n",
       "275135                 0.002785                 0.0             0.000050   \n",
       "232502                 0.005392                 1.0             0.000343   \n",
       "\n",
       "        malware_xgboost_result  dga_binary_nn_result  phishing_sum  \\\n",
       "54451                 0.002436          6.067854e-23      1.092264   \n",
       "247717                0.002233          3.490693e-03      0.011647   \n",
       "15621                 0.002408          7.239705e-07      5.393736   \n",
       "255533                0.002364          1.192370e-35      0.057303   \n",
       "89645                 0.002233          5.018924e-04      0.107293   \n",
       "...                        ...                   ...           ...   \n",
       "343814                0.002233          3.854592e-02      1.005146   \n",
       "104123                0.002364          8.776547e-26      0.157121   \n",
       "380872                0.568785          2.559731e-02      0.105889   \n",
       "275135                0.002233          7.275866e-01      0.015347   \n",
       "232502                0.002364          9.142273e-07      1.020225   \n",
       "\n",
       "        phishing_avg  phishing_prod  malware_sum  malware_avg  malware_prod  \\\n",
       "54451       0.182044   2.825370e-11     1.002627     0.334209  4.655318e-07   \n",
       "247717      0.001941   0.000000e+00     0.002285     0.000762  0.000000e+00   \n",
       "15621       0.898956   4.994589e-01     0.002941     0.000980  0.000000e+00   \n",
       "255533      0.009551   0.000000e+00     0.002644     0.000881  0.000000e+00   \n",
       "89645       0.017882   0.000000e+00     0.002294     0.000765  0.000000e+00   \n",
       "...              ...            ...          ...          ...           ...   \n",
       "343814      0.167524   6.602678e-17     1.002286     0.334095  1.171542e-07   \n",
       "104123      0.026187   0.000000e+00     1.002669     0.334223  7.199457e-07   \n",
       "380872      0.017648   0.000000e+00     1.246605     0.415535  0.000000e+00   \n",
       "275135      0.002558   0.000000e+00     0.002283     0.000761  0.000000e+00   \n",
       "232502      0.170037   4.292383e-13     1.002707     0.334236  8.102686e-07   \n",
       "\n",
       "        total_sum  total_avg    total_prod  \n",
       "54451    2.094891   0.209489  7.981044e-40  \n",
       "247717   0.017423   0.001742  0.000000e+00  \n",
       "15621    5.396678   0.539668  0.000000e+00  \n",
       "255533   0.059947   0.005995  0.000000e+00  \n",
       "89645    0.110089   0.011009  0.000000e+00  \n",
       "...           ...        ...           ...  \n",
       "343814   2.045977   0.204598  2.981647e-25  \n",
       "104123   1.159789   0.115979  0.000000e+00  \n",
       "380872   1.378092   0.137809  0.000000e+00  \n",
       "275135   0.745217   0.074522  0.000000e+00  \n",
       "232502   2.022932   0.202293  3.179667e-25  \n",
       "\n",
       "[100000 rows x 29 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from pyarrow import Table\n",
    "\n",
    "df = pd.read_parquet('data/preliminary_results_x.parquet')\n",
    "\n",
    "# IMPORTANT: set labels:\n",
    "df.loc[df['label'].str.startswith(('phishing', 'malware', 'misp', 'dga')), 'label'] = 'malign'\n",
    "df.loc[df['label'] != 'malign', 'label'] = 'benign'\n",
    "\n",
    "class_map = {\"benign\": 0, \"malign\": 1}\n",
    "\n",
    "# Handle NaNs\n",
    "df.fillna(-1, inplace=True)\n",
    "\n",
    "\n",
    "# SUBSAMPLE (OPTIONAL)\n",
    "subsample = 1.0 # 1.0 means no subsample\n",
    "if subsample < 1.0:\n",
    "    df = df.sample(frac=subsample)\n",
    "\n",
    "# Drop the domain name column\n",
    "df.drop(\"domain_name\", axis=1, inplace=True)\n",
    "\n",
    "labels = df['label'].apply(lambda x: class_map[x]) # y vector\n",
    "features = df.drop('label', axis=1).copy() # X matrix\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e103a79",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87705691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dns_available</th>\n",
       "      <th>dns_nonzero</th>\n",
       "      <th>tls_available</th>\n",
       "      <th>tls_nonzero</th>\n",
       "      <th>ip_available</th>\n",
       "      <th>ip_nonzero</th>\n",
       "      <th>rdap_available</th>\n",
       "      <th>rdap_nonzero</th>\n",
       "      <th>geo_available</th>\n",
       "      <th>geo_nonzero</th>\n",
       "      <th>phishing_cnn_result</th>\n",
       "      <th>phishing_lgbm_result</th>\n",
       "      <th>phishing_xgboost_result</th>\n",
       "      <th>phishing_deepnn_result</th>\n",
       "      <th>phishing_dns_nn_result</th>\n",
       "      <th>phishing_rdap_nn_result</th>\n",
       "      <th>malware_cnn_result</th>\n",
       "      <th>malware_lgbm_result</th>\n",
       "      <th>malware_xgboost_result</th>\n",
       "      <th>dga_binary_nn_result</th>\n",
       "      <th>phishing_sum</th>\n",
       "      <th>phishing_avg</th>\n",
       "      <th>phishing_prod</th>\n",
       "      <th>malware_sum</th>\n",
       "      <th>malware_avg</th>\n",
       "      <th>malware_prod</th>\n",
       "      <th>total_sum</th>\n",
       "      <th>total_avg</th>\n",
       "      <th>total_prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>1.366864e-03</td>\n",
       "      <td>0.066430</td>\n",
       "      <td>0.013656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>6.067854e-23</td>\n",
       "      <td>0.182268</td>\n",
       "      <td>0.182268</td>\n",
       "      <td>2.856502e-11</td>\n",
       "      <td>0.346461</td>\n",
       "      <td>0.346461</td>\n",
       "      <td>5.223303e-07</td>\n",
       "      <td>0.229910</td>\n",
       "      <td>0.229910</td>\n",
       "      <td>2.206768e-39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>3.579364e-04</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.490693e-03</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.794877</td>\n",
       "      <td>0.969108</td>\n",
       "      <td>9.999437e-01</td>\n",
       "      <td>0.690715</td>\n",
       "      <td>0.943112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>7.239705e-07</td>\n",
       "      <td>0.900585</td>\n",
       "      <td>0.900585</td>\n",
       "      <td>5.049623e-01</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.592965</td>\n",
       "      <td>0.592965</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>3.409265e-07</td>\n",
       "      <td>0.055762</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>1.192370e-35</td>\n",
       "      <td>0.009436</td>\n",
       "      <td>0.009436</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.006152</td>\n",
       "      <td>0.006152</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>6.330108e-04</td>\n",
       "      <td>0.071380</td>\n",
       "      <td>0.033853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.018924e-04</td>\n",
       "      <td>0.017784</td>\n",
       "      <td>0.017784</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.011666</td>\n",
       "      <td>0.011666</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>1.249553e-04</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.854592e-02</td>\n",
       "      <td>0.167720</td>\n",
       "      <td>0.167720</td>\n",
       "      <td>6.675432e-17</td>\n",
       "      <td>0.346342</td>\n",
       "      <td>0.346342</td>\n",
       "      <td>1.314479e-07</td>\n",
       "      <td>0.224531</td>\n",
       "      <td>0.224531</td>\n",
       "      <td>8.244289e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>2.489036e-04</td>\n",
       "      <td>0.144864</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>8.776547e-26</td>\n",
       "      <td>0.026105</td>\n",
       "      <td>0.026105</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.346475</td>\n",
       "      <td>0.346475</td>\n",
       "      <td>8.077846e-07</td>\n",
       "      <td>0.127088</td>\n",
       "      <td>0.127088</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>1.054552e-02</td>\n",
       "      <td>0.051355</td>\n",
       "      <td>0.027223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.684465</td>\n",
       "      <td>0.585975</td>\n",
       "      <td>2.559731e-02</td>\n",
       "      <td>0.017550</td>\n",
       "      <td>0.017550</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.430960</td>\n",
       "      <td>0.430960</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.151092</td>\n",
       "      <td>0.151092</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>2.749782e-03</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.275866e-01</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.081503</td>\n",
       "      <td>0.081503</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>3.551183e-03</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>9.142273e-07</td>\n",
       "      <td>0.170238</td>\n",
       "      <td>0.170238</td>\n",
       "      <td>4.339680e-13</td>\n",
       "      <td>0.346488</td>\n",
       "      <td>0.346488</td>\n",
       "      <td>9.091277e-07</td>\n",
       "      <td>0.221997</td>\n",
       "      <td>0.221997</td>\n",
       "      <td>8.791817e-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dns_available  dns_nonzero  tls_available  tls_nonzero  ip_available  \\\n",
       "0                0.6     0.257143       1.000000     0.952381           1.0   \n",
       "1                1.0     0.857143       0.833333     0.333333           1.0   \n",
       "2                0.6     0.257143       1.000000     0.952381           1.0   \n",
       "3                0.9     0.600000       0.833333     0.333333           1.0   \n",
       "4                0.9     0.628571       0.833333     0.333333           1.0   \n",
       "...              ...          ...            ...          ...           ...   \n",
       "99995            1.0     0.800000       0.833333     0.333333           1.0   \n",
       "99996            0.6     0.228571       1.000000     0.952381           1.0   \n",
       "99997            1.0     0.828571       0.833333     0.333333           1.0   \n",
       "99998            1.0     0.800000       0.833333     0.333333           1.0   \n",
       "99999            0.6     0.285714       1.000000     0.809524           1.0   \n",
       "\n",
       "       ip_nonzero  rdap_available  rdap_nonzero  geo_available  geo_nonzero  \\\n",
       "0           0.750        1.000000      0.833333            1.0     0.777778   \n",
       "1           1.000        1.000000      0.708333            1.0     1.000000   \n",
       "2           0.375        1.000000      0.708333            1.0     0.722222   \n",
       "3           0.875        0.826087      0.250000            1.0     0.722222   \n",
       "4           1.000        1.000000      0.708333            1.0     1.000000   \n",
       "...           ...             ...           ...            ...          ...   \n",
       "99995       1.000        0.826087      0.416667            1.0     1.000000   \n",
       "99996       0.500        0.826087      0.041667            1.0     0.666667   \n",
       "99997       0.625        1.000000      0.708333            1.0     0.722222   \n",
       "99998       1.000        1.000000      0.708333            1.0     1.000000   \n",
       "99999       0.500        0.869565      0.208333            1.0     0.666667   \n",
       "\n",
       "       phishing_cnn_result  phishing_lgbm_result  phishing_xgboost_result  \\\n",
       "0                      1.0              0.007990                 0.002456   \n",
       "1                      0.0              0.000549                 0.000258   \n",
       "2                      1.0              0.794877                 0.969108   \n",
       "3                      0.0              0.000181                 0.000018   \n",
       "4                      0.0              0.000535                 0.000571   \n",
       "...                    ...                   ...                      ...   \n",
       "99995                  1.0              0.000221                 0.000268   \n",
       "99996                  0.0              0.000387                 0.000415   \n",
       "99997                  0.0              0.006618                 0.009798   \n",
       "99998                  0.0              0.000293                 0.000177   \n",
       "99999                  1.0              0.000866                 0.002786   \n",
       "\n",
       "       phishing_deepnn_result  phishing_dns_nn_result  \\\n",
       "0                1.366864e-03                0.066430   \n",
       "1                3.579364e-04                0.007211   \n",
       "2                9.999437e-01                0.690715   \n",
       "3                3.409265e-07                0.055762   \n",
       "4                6.330108e-04                0.071380   \n",
       "...                       ...                     ...   \n",
       "99995            1.249553e-04                0.003328   \n",
       "99996            2.489036e-04                0.144864   \n",
       "99997            1.054552e-02                0.051355   \n",
       "99998            2.749782e-03                0.008861   \n",
       "99999            3.551183e-03                0.007148   \n",
       "\n",
       "       phishing_rdap_nn_result  malware_cnn_result  malware_lgbm_result  \\\n",
       "0                     0.013656                 1.0             0.000149   \n",
       "1                     0.002792                 0.0             0.000009   \n",
       "2                     0.943112                 0.0             0.000494   \n",
       "3                     0.000931                 0.0             0.000238   \n",
       "4                     0.033853                 0.0             0.000017   \n",
       "...                        ...                 ...                  ...   \n",
       "99995                 0.000715                 1.0             0.000009   \n",
       "99996                 0.010949                 1.0             0.000264   \n",
       "99997                 0.027223                 0.0             0.684465   \n",
       "99998                 0.002791                 0.0             0.000007   \n",
       "99999                 0.005403                 1.0             0.000302   \n",
       "\n",
       "       malware_xgboost_result  dga_binary_nn_result  phishing_sum  \\\n",
       "0                    0.000209          6.067854e-23      0.182268   \n",
       "1                    0.000000          3.490693e-03      0.001812   \n",
       "2                    0.000181          7.239705e-07      0.900585   \n",
       "3                    0.000135          1.192370e-35      0.009436   \n",
       "4                    0.000000          5.018924e-04      0.017784   \n",
       "...                       ...                   ...           ...   \n",
       "99995                0.000000          3.854592e-02      0.167720   \n",
       "99996                0.000135          8.776547e-26      0.026105   \n",
       "99997                0.585975          2.559731e-02      0.017550   \n",
       "99998                0.000000          7.275866e-01      0.002430   \n",
       "99999                0.000135          9.142273e-07      0.170238   \n",
       "\n",
       "       phishing_avg  phishing_prod  malware_sum  malware_avg  malware_prod  \\\n",
       "0          0.182268   2.856502e-11     0.346461     0.346461  5.223303e-07   \n",
       "1          0.001812   0.000000e+00     0.000003     0.000003  0.000000e+00   \n",
       "2          0.900585   5.049623e-01     0.000230     0.000230  0.000000e+00   \n",
       "3          0.009436   0.000000e+00     0.000127     0.000127  0.000000e+00   \n",
       "4          0.017784   0.000000e+00     0.000006     0.000006  0.000000e+00   \n",
       "...             ...            ...          ...          ...           ...   \n",
       "99995      0.167720   6.675432e-17     0.346342     0.346342  1.314479e-07   \n",
       "99996      0.026105   0.000000e+00     0.346475     0.346475  8.077846e-07   \n",
       "99997      0.017550   0.000000e+00     0.430960     0.430960  0.000000e+00   \n",
       "99998      0.002430   0.000000e+00     0.000002     0.000002  0.000000e+00   \n",
       "99999      0.170238   4.339680e-13     0.346488     0.346488  9.091277e-07   \n",
       "\n",
       "       total_sum  total_avg    total_prod  \n",
       "0       0.229910   0.229910  2.206768e-39  \n",
       "1       0.001477   0.001477  0.000000e+00  \n",
       "2       0.592965   0.592965  0.000000e+00  \n",
       "3       0.006152   0.006152  0.000000e+00  \n",
       "4       0.011666   0.011666  0.000000e+00  \n",
       "...          ...        ...           ...  \n",
       "99995   0.224531   0.224531  8.244289e-25  \n",
       "99996   0.127088   0.127088  0.000000e+00  \n",
       "99997   0.151092   0.151092  0.000000e+00  \n",
       "99998   0.081503   0.081503  0.000000e+00  \n",
       "99999   0.221997   0.221997  8.791817e-25  \n",
       "\n",
       "[100000 rows x 29 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    " \n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(features)\n",
    "features = pd.DataFrame(scaled_data, columns=features.columns)\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, \"boundaries/decision_nn_scaler.joblib\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fd630",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae4bf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "  features,\n",
    "  labels,\n",
    "  test_size=0.2,\n",
    "  random_state=42,\n",
    "  shuffle=True, \n",
    "  stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecd4d4",
   "metadata": {},
   "source": [
    "# Define the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96ecaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, feature_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "\n",
    "        # Adjust the size calculation based on the number of convolutional layers\n",
    "        self.fc1 = nn.Linear(feature_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        \n",
    "        # Optionally use dropout\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(F.relu(self.fc2(x)))\n",
    "        x = self.dropout1(F.relu(self.fc3(x)))\n",
    "\n",
    "        return self.fc4(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2a2901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(data_loader, model):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            true_labels.extend(target.cpu().numpy())\n",
    "            predictions.extend(torch.sigmoid(output).round().cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    return accuracy, f1, predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c43e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "x_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "\n",
    "print(\"Feature size:\", X_train.shape[1])\n",
    "print(\"Training samples:\", len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f05d4",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cb708ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Loss: 0.0531, Accuracy: 0.9671, F1 Score: 0.9487\n",
      "Epoch 2/15 - Loss: 0.0411, Accuracy: 0.9673, F1 Score: 0.9490\n",
      "Epoch 3/15 - Loss: 0.0383, Accuracy: 0.9684, F1 Score: 0.9508\n",
      "Epoch 4/15 - Loss: 0.0373, Accuracy: 0.9670, F1 Score: 0.9483\n",
      "Epoch 5/15 - Loss: 0.0364, Accuracy: 0.9714, F1 Score: 0.9557\n",
      "Epoch 6/15 - Loss: 0.0358, Accuracy: 0.9750, F1 Score: 0.9624\n",
      "Epoch 7/15 - Loss: 0.0355, Accuracy: 0.9739, F1 Score: 0.9598\n",
      "Epoch 8/15 - Loss: 0.0349, Accuracy: 0.9713, F1 Score: 0.9555\n",
      "Epoch 9/15 - Loss: 0.0346, Accuracy: 0.9690, F1 Score: 0.9516\n",
      "Epoch 10/15 - Loss: 0.0341, Accuracy: 0.9742, F1 Score: 0.9604\n",
      "Epoch 11/15 - Loss: 0.0343, Accuracy: 0.9688, F1 Score: 0.9514\n",
      "Epoch 12/15 - Loss: 0.0339, Accuracy: 0.9706, F1 Score: 0.9543\n",
      "Epoch 13/15 - Loss: 0.0336, Accuracy: 0.9733, F1 Score: 0.9587\n",
      "Epoch 14/15 - Loss: 0.0335, Accuracy: 0.9701, F1 Score: 0.9535\n",
      "Epoch 15/15 - Loss: 0.0334, Accuracy: 0.9731, F1 Score: 0.9585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.0045\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "# # Calculate class weights\n",
    "class_weights = {0: 1.0, 1: 0.3} \n",
    "weights = torch.tensor([class_weights[1]], dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "model = Net(x_train.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_data = TensorDataset(x_train.to(device), y_train.float().unsqueeze(1).to(device))  # Ensure y_train is float and of shape (batch_size, 1)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(x_test.to(device), y_test.float().unsqueeze(1).to(device))  # Ensure y_test is float and of shape (batch_size, 1)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "epoch_losses = []\n",
    "epoch_accuracies = []\n",
    "epoch_f1s = []\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)  # Move data to the device\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        # negate the output to match the class weights\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "\n",
    "    # Evaluate model and store metrics\n",
    "    train_accuracy, train_f1, _, _ = compute_metrics(train_loader, model)\n",
    "    epoch_accuracies.append(train_accuracy)\n",
    "    epoch_f1s.append(train_f1)\n",
    "\n",
    "    # Enhanced logging\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.4f}, F1 Score: {train_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9dc28",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d75acc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model \n",
    "torch.save(model.state_dict(), './models/decision_malware.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59898f79",
   "metadata": {},
   "source": [
    "# Display training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81a03016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66750/3566102466.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./models/decision_malware.pth'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Net:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([512, 29]) from checkpoint, the shape in current model is torch.Size([512, 173]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# load model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Net(feature_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m173\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/decision_malware.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set\u001b[39;00m\n\u001b[1;32m      9\u001b[0m test_accuracy, test_f1, predictions, true_labels \u001b[38;5;241m=\u001b[39m compute_metrics(test_loader, model)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/domainradar-training-YTp98ywL-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Net:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([512, 29]) from checkpoint, the shape in current model is torch.Size([512, 173])."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# load model\n",
    "model = Net(feature_size=29).to(device)\n",
    "model.load_state_dict(torch.load('./models/decision_malware.pth'))\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy, test_f1, predictions, true_labels = compute_metrics(test_loader, model)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix - Test Data')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot for Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epoch_losses, linestyle='--', marker='o', color='#2ba7fc', label=f'Loss (Best: {min(epoch_losses):.4f})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epoch_accuracies, linestyle='--', marker='o', color='#61d484', label=f'Accuracy (Best: {max(epoch_accuracies):.4f})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for F1 Score\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epoch_f1s, linestyle='--', marker='o', color='#b85e4f', label=f'F1 Score (Best: {max(epoch_f1s):.4f})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Training F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle('Training Progress')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the testing results\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "metrics = ['Accuracy', 'F1 Score']\n",
    "values = [test_accuracy, test_f1]\n",
    "colors = ['#61d484', '#b85e4f']\n",
    "\n",
    "plt.bar(metrics, values, color=colors)\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v + 0.02, f\"{v:.4f}\", ha='center', va='bottom')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Test Metrics')\n",
    "plt.show()\n",
    "\n",
    "# bylo 2k na 2k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6408a68",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate the model\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test)\n",
    "print('Loss = ', loss_and_metrics[0])\n",
    "print('Accuracy = ', loss_and_metrics[1])\n",
    "\n",
    "# Generate predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred = np.round(Y_pred).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(Y_test, Y_pred)\n",
    "recall = recall_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "# False Positive Rate\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "# Display the metrics\n",
    "print('\\n=== RESULTS ===')\n",
    "print(classification_report(Y_test, Y_pred, target_names=['Benign', 'Malicious'], digits=4))\n",
    "print('False Positive Rate =', fpr)\n",
    "\n",
    "\n",
    "# Display the confusion matrix\n",
    "print('\\nConfusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Optionally, plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "fig, ax = plt.subplots(figsize=(7, 7))  # Increase figure size for better readability\n",
    "disp.plot(ax=ax, values_format='d')\n",
    "for labels in disp.text_:\n",
    "    for label in labels:\n",
    "        label.set_fontsize(18) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5605fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09828f28",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Ensure that X_train and X_test are DataFrames with the correct column names\n",
    "# You can set the column names from the 'features' DataFrame like this:\n",
    "X_train.columns = features.columns\n",
    "X_test.columns = features.columns\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "# Convert your training set to a NumPy format if it's not already\n",
    "background = X_train[:n_samples].to_numpy()\n",
    "\n",
    "# Use the generic SHAP Explainer interface\n",
    "explainer = shap.Explainer(model, background)\n",
    "\n",
    "# Generate SHAP values for the test set\n",
    "shap_values = explainer(X_test[:n_samples].to_numpy())\n",
    "\n",
    "# Plotting the summary plot for feature importance\n",
    "# Use the column names from the 'features' DataFrame as the feature names\n",
    "shap.summary_plot(shap_values.values, X_test[:n_samples], feature_names=features.columns, max_display=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a3593",
   "metadata": {},
   "source": [
    "# Make test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from pyarrow import Table\n",
    "\n",
    "# Load the model and scaler\n",
    "model = load_model('dga_binary_model.keras')\n",
    "scaler = joblib.load(\"dga_binary_scaler.joblib\")\n",
    "\n",
    "# #############################################################\n",
    "# EDIT this to specify benign / malicious datasets to use     #\n",
    "# #############################################################\n",
    "benign_dataset_filenames = [\n",
    "    '../feature-extraction/floor/lex-benign_2312_anonymized.parquet',\n",
    "    '../feature-extraction/floor/lex-umbrella_benign_FINISHED.parquet',\n",
    "]\n",
    "malicious_dataset_filenames = [\n",
    "    '../feature-extraction/floor/lex-dga-830k-pick.parquet'\n",
    "]\n",
    "# #############################################################\n",
    "# EDIT this for to set appropriate labels (malware, dga, ...) #\n",
    "# #############################################################\n",
    "benign_label = \"benign\"\n",
    "malicious_label = \"dga\"\n",
    "# #############################################################\n",
    "\n",
    "def union_tables(tables: [pa.Table]) -> pa.Table:\n",
    "    union_table = tables[0]\n",
    "    for table in tables[1:]:\n",
    "        right_not_in_union = union_table.join(right_table=table, keys='domain_name', join_type='right anti',\n",
    "                                              coalesce_keys=True, use_threads=True)\n",
    "        union_table = pa.concat_tables([union_table, right_not_in_union])\n",
    "    return union_table\n",
    "\n",
    "# Unify malicious datasets and benign datasets\n",
    "schema = (pq.read_table(malicious_dataset_filenames[0])).schema # Use the schema from the first malicious filename\n",
    "benign_tables = [pq.read_table(filename).cast(schema) for filename in benign_dataset_filenames]\n",
    "malicious_tables = [pq.read_table(filename).cast(schema) for filename in malicious_dataset_filenames]\n",
    "malicious = union_tables(malicious_tables)\n",
    "benign = union_tables(benign_tables)\n",
    "\n",
    "# Convert pyarrow tables to pandas dataframes\n",
    "df_benign = benign.to_pandas()\n",
    "df_malicious = malicious.to_pandas()\n",
    "\n",
    "# Set appropriate labels\n",
    "df_benign[\"label\"] = benign_label\n",
    "df_malicious[\"label\"] = malicious_label\n",
    "class_map = {benign_label: 0, malicious_label: 1}\n",
    "\n",
    "# Concatentate benign and malicious\n",
    "test_df = pd.concat([df_benign, df_malicious])\n",
    "\n",
    "# Handle NaNs\n",
    "test_df.fillna(-1, inplace=True)\n",
    "\n",
    "\n",
    "# Take only N random samples\n",
    "N = 500\n",
    "test_df = test_df.sample(n=N, random_state=42)\n",
    "\n",
    "total_predictions = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    domain_name = row['domain_name']\n",
    "    original_label = row['label']\n",
    "    \n",
    "    # Drop \"domain_name\" and \"label\" columns\n",
    "    feature_vector = pd.DataFrame([row])\n",
    "    feature_vector.drop(columns=['domain_name', 'label'], inplace=True)\n",
    "\n",
    "     # Scale the feature vector using the loaded scaler\n",
    "    scaled_feature_vector = scaler.transform(feature_vector)\n",
    "    \n",
    "    # Perform prediction\n",
    "    prediction = model.predict(scaled_feature_vector, verbose=0)\n",
    "    \n",
    "    # Extract the predicted class\n",
    "    predicted_label = \"benign\" if prediction < 0.5 else \"dga\"\n",
    "    \n",
    "    # Check if the prediction was correct\n",
    "    if original_label == predicted_label:\n",
    "        correct_predictions += 1\n",
    "    \n",
    "    total_predictions += 1\n",
    "    \n",
    "    # Print the result\n",
    "    result=\"WRONG\"\n",
    "    if predicted_label == original_label:\n",
    "        result=\"OK\"\n",
    "        \n",
    "    pred_disp = \"!!! DGA !!!\"\n",
    "    if predicted_label == \"benign\":\n",
    "        pred_disp = \"BENIGN\"\n",
    "        \n",
    "    \n",
    "    print(f\"{result} | {domain_name} ({original_label}), Predicted: {pred_disp}, Prob: {prediction}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3d6c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e63ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206433e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47d300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
