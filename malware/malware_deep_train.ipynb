{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da08d06-be0d-468a-bd17-d7bcba74880a",
   "metadata": {},
   "source": [
    "## Columns to be removed from training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94277f7d-8c84-4e29-a892-780785afdb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "disqualified_columns = [\"tls_joint_isoitu_policy_crt_count\", \"rdap_time_from_last_change\", \"lex_www_flag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b715b0f7",
   "metadata": {},
   "source": [
    "# Load Tensorflow and check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a3b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import sys\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1258a0e",
   "metadata": {},
   "source": [
    "# Load input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "069058d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign columns: 178\n",
      "Malicious columns: 178\n",
      "Total samples: 626617\n",
      "Benign count: 462192\n",
      "Malicious count: 164425\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.core.dtypes import common as com\n",
    "from pyarrow import Table\n",
    "\n",
    "\n",
    "def union_tables(tables: [pa.Table]) -> pa.Table:\n",
    "    union_table = tables[0]\n",
    "    for table in tables[1:]:\n",
    "        right_not_in_union = union_table.join(right_table=table, keys='domain_name', join_type='right anti',\n",
    "                                              coalesce_keys=True, use_threads=True)\n",
    "        union_table = pa.concat_tables([union_table, right_not_in_union])\n",
    "    return union_table\n",
    "\n",
    "# #############################################################\n",
    "# EDIT this to specify benign / malicious datasets to use     #\n",
    "# #############################################################\n",
    "benign_dataset_filenames = [\n",
    "    '../feature-extraction/floor/benign_2312.parquet',\n",
    "]\n",
    "malicious_dataset_filenames = [\n",
    "    '../feature-extraction/floor/phishing_2406_strict.parquet'\n",
    "]\n",
    "# #############################################################\n",
    "# EDIT this for to set appropriate labels (malware, dga, ...) #\n",
    "# #############################################################\n",
    "benign_label = \"benign\"\n",
    "malicious_label = \"phishing\"\n",
    "# #############################################################\n",
    "\n",
    "\n",
    "\n",
    "# Unify malicious datasets and benign datasets\n",
    "schema = (pq.read_table(malicious_dataset_filenames[0])).schema # Use the schema from the first malicious filename\n",
    "benign_tables = [pq.read_table(filename).cast(schema) for filename in benign_dataset_filenames]\n",
    "malicious_tables = [pq.read_table(filename).cast(schema) for filename in malicious_dataset_filenames]\n",
    "malicious = union_tables(malicious_tables)\n",
    "benign = union_tables(benign_tables)\n",
    "\n",
    "# Convert pyarrow tables to pandas dataframes\n",
    "df_benign = benign.to_pandas()\n",
    "df_malicious = malicious.to_pandas()\n",
    "\n",
    "# Set appropriate labels\n",
    "df_benign[\"label\"] = benign_label\n",
    "df_malicious[\"label\"] = malicious_label\n",
    "class_map = {benign_label: 0, malicious_label: 1}\n",
    "\n",
    "# print column count\n",
    "print(f\"Benign columns: {len(df_benign.columns)}\")\n",
    "print(f\"Malicious columns: {len(df_malicious.columns)}\")\n",
    "\n",
    "\n",
    "\n",
    "# ===================\n",
    "# AUTO BALANCING !!!\n",
    "# Subsample benign to match the size of malicious\n",
    "# df_benign = df_benign.sample(n=len(df_malicious))\n",
    "# ===================\n",
    "\n",
    "# Concatentate benign and malicious\n",
    "df = pd.concat([df_benign, df_malicious])\n",
    "\n",
    "\n",
    "def cast_timestamp(df: DataFrame):\n",
    "    \"\"\"\n",
    "    Cast timestamp fields to seconds since epoch.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if com.is_timedelta64_dtype(df[col]):\n",
    "            df[col] = df[col].dt.total_seconds()  # This converts timedelta to float (seconds)\n",
    "        elif com.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = df[col].astype(np.int64) // 10**9  # Converts datetime64 to Unix timestamp (seconds)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = cast_timestamp(df)\n",
    "\n",
    "# Handle NaNs\n",
    "df.fillna(-1, inplace=True)\n",
    "\n",
    "\n",
    "# SUBSAMPLE (OPTIONAL)\n",
    "subsample = 1.0 # 1.0 means no subsample\n",
    "if subsample < 1.0:\n",
    "    df = df.sample(frac=subsample)\n",
    "\n",
    "# Drop the domain name column\n",
    "df.drop(\"domain_name\", axis=1, inplace=True)\n",
    "\n",
    "# Remove disqualified columns\n",
    "for column in disqualified_columns:\n",
    "    if column in df.columns:\n",
    "        df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "labels = df['label'].apply(lambda x: class_map[x]) # y vector\n",
    "features = df.drop('label', axis=1).copy() # X matrix\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Benign count: {len(df_benign)}\")\n",
    "print(f\"Malicious count: {len(df_malicious)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e103a79",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "87705691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    " \n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(features)\n",
    "features = pd.DataFrame(scaled_data, columns=features.columns)\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, \"boundaries/malware_general_scaler.joblib\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fd630",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ae4bf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "  features,\n",
    "  labels,\n",
    "  test_size=0.2,\n",
    "  random_state=42,\n",
    "  shuffle=True, \n",
    "  stratify=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecd4d4",
   "metadata": {},
   "source": [
    "# Define the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ecaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, feature_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "\n",
    "        # Adjust the size calculation based on the number of convolutional layers\n",
    "        self.fc1 = nn.Linear(feature_size, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        \n",
    "        # Optionally use dropout\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(F.relu(self.fc2(x)))\n",
    "        x = self.dropout1(F.relu(self.fc3(x)))\n",
    "\n",
    "        return self.fc4(x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e39b63c-49a3-43f1-a1a9-b0a99892385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(data_loader, model):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            true_labels.extend(target.cpu().numpy())\n",
    "            predictions.extend(torch.sigmoid(output).round().cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    return accuracy, f1, predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a587cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def prepare_dataset(X_train, Y_train, X_test, Y_test):\n",
    "\n",
    "    # Convert data to torch tensors\n",
    "    x_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "    x_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    y_test = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, side_size\n",
    "\n",
    "\n",
    "# print feature size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c6111038",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, side_size = prepare_dataset(X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "#x_train = torch.cat((x_train, pickle.load(open('old_phishing_x.pkl', 'rb'))))\n",
    "#y_train = torch.cat((y_train, pickle.load(open('old_phishing_y.pkl', 'rb'))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f05d4",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5cb708ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model created\n",
      "Epoch 1/80 - Loss: 0.1106, Accuracy: 0.9719, F1 Score: 0.9466\n",
      "Epoch 2/80 - Loss: 0.0762, Accuracy: 0.9800, F1 Score: 0.9611\n",
      "Epoch 3/80 - Loss: 0.0665, Accuracy: 0.9809, F1 Score: 0.9628\n",
      "Epoch 4/80 - Loss: 0.0609, Accuracy: 0.9823, F1 Score: 0.9654\n",
      "Epoch 5/80 - Loss: 0.0566, Accuracy: 0.9847, F1 Score: 0.9703\n",
      "Epoch 6/80 - Loss: 0.0528, Accuracy: 0.9837, F1 Score: 0.9680\n",
      "Epoch 7/80 - Loss: 0.0501, Accuracy: 0.9861, F1 Score: 0.9730\n",
      "Epoch 8/80 - Loss: 0.0472, Accuracy: 0.9856, F1 Score: 0.9725\n",
      "Epoch 9/80 - Loss: 0.0454, Accuracy: 0.9864, F1 Score: 0.9739\n",
      "Epoch 10/80 - Loss: 0.0429, Accuracy: 0.9876, F1 Score: 0.9762\n",
      "Epoch 11/80 - Loss: 0.0408, Accuracy: 0.9889, F1 Score: 0.9786\n",
      "Epoch 12/80 - Loss: 0.0387, Accuracy: 0.9897, F1 Score: 0.9802\n",
      "Epoch 13/80 - Loss: 0.0369, Accuracy: 0.9900, F1 Score: 0.9807\n",
      "Epoch 14/80 - Loss: 0.0353, Accuracy: 0.9906, F1 Score: 0.9819\n",
      "Epoch 15/80 - Loss: 0.0331, Accuracy: 0.9905, F1 Score: 0.9818\n",
      "Epoch 16/80 - Loss: 0.0317, Accuracy: 0.9901, F1 Score: 0.9809\n",
      "Epoch 17/80 - Loss: 0.0299, Accuracy: 0.9916, F1 Score: 0.9837\n",
      "Epoch 18/80 - Loss: 0.0283, Accuracy: 0.9921, F1 Score: 0.9847\n",
      "Epoch 19/80 - Loss: 0.0268, Accuracy: 0.9925, F1 Score: 0.9856\n",
      "Epoch 20/80 - Loss: 0.0253, Accuracy: 0.9930, F1 Score: 0.9866\n",
      "Epoch 21/80 - Loss: 0.0241, Accuracy: 0.9914, F1 Score: 0.9837\n",
      "Epoch 22/80 - Loss: 0.0228, Accuracy: 0.9937, F1 Score: 0.9880\n",
      "Epoch 23/80 - Loss: 0.0210, Accuracy: 0.9944, F1 Score: 0.9892\n",
      "Epoch 24/80 - Loss: 0.0200, Accuracy: 0.9949, F1 Score: 0.9903\n",
      "Epoch 25/80 - Loss: 0.0185, Accuracy: 0.9953, F1 Score: 0.9909\n",
      "Epoch 26/80 - Loss: 0.0179, Accuracy: 0.9956, F1 Score: 0.9915\n",
      "Epoch 27/80 - Loss: 0.0165, Accuracy: 0.9954, F1 Score: 0.9912\n",
      "Epoch 28/80 - Loss: 0.0157, Accuracy: 0.9957, F1 Score: 0.9917\n",
      "Epoch 29/80 - Loss: 0.0147, Accuracy: 0.9958, F1 Score: 0.9920\n",
      "Epoch 30/80 - Loss: 0.0140, Accuracy: 0.9963, F1 Score: 0.9929\n",
      "Epoch 31/80 - Loss: 0.0133, Accuracy: 0.9964, F1 Score: 0.9930\n",
      "Epoch 32/80 - Loss: 0.0126, Accuracy: 0.9968, F1 Score: 0.9940\n",
      "Epoch 33/80 - Loss: 0.0120, Accuracy: 0.9972, F1 Score: 0.9947\n",
      "Epoch 34/80 - Loss: 0.0117, Accuracy: 0.9962, F1 Score: 0.9926\n",
      "Epoch 35/80 - Loss: 0.0106, Accuracy: 0.9970, F1 Score: 0.9943\n",
      "Epoch 36/80 - Loss: 0.0105, Accuracy: 0.9967, F1 Score: 0.9936\n",
      "Epoch 37/80 - Loss: 0.0097, Accuracy: 0.9972, F1 Score: 0.9946\n",
      "Epoch 38/80 - Loss: 0.0095, Accuracy: 0.9975, F1 Score: 0.9953\n",
      "Epoch 39/80 - Loss: 0.0091, Accuracy: 0.9967, F1 Score: 0.9937\n",
      "Epoch 40/80 - Loss: 0.0087, Accuracy: 0.9979, F1 Score: 0.9960\n",
      "Epoch 41/80 - Loss: 0.0085, Accuracy: 0.9980, F1 Score: 0.9961\n",
      "Epoch 42/80 - Loss: 0.0082, Accuracy: 0.9982, F1 Score: 0.9966\n",
      "Epoch 43/80 - Loss: 0.0079, Accuracy: 0.9982, F1 Score: 0.9966\n",
      "Epoch 44/80 - Loss: 0.0074, Accuracy: 0.9981, F1 Score: 0.9964\n",
      "Epoch 45/80 - Loss: 0.0074, Accuracy: 0.9979, F1 Score: 0.9960\n",
      "Epoch 46/80 - Loss: 0.0071, Accuracy: 0.9985, F1 Score: 0.9971\n",
      "Epoch 47/80 - Loss: 0.0067, Accuracy: 0.9981, F1 Score: 0.9963\n",
      "Epoch 48/80 - Loss: 0.0069, Accuracy: 0.9985, F1 Score: 0.9971\n",
      "Epoch 49/80 - Loss: 0.0066, Accuracy: 0.9985, F1 Score: 0.9971\n",
      "Epoch 50/80 - Loss: 0.0062, Accuracy: 0.9983, F1 Score: 0.9968\n",
      "Epoch 51/80 - Loss: 0.0063, Accuracy: 0.9980, F1 Score: 0.9962\n",
      "Epoch 52/80 - Loss: 0.0058, Accuracy: 0.9983, F1 Score: 0.9968\n",
      "Epoch 53/80 - Loss: 0.0060, Accuracy: 0.9985, F1 Score: 0.9972\n",
      "Epoch 54/80 - Loss: 0.0056, Accuracy: 0.9983, F1 Score: 0.9967\n",
      "Epoch 55/80 - Loss: 0.0055, Accuracy: 0.9986, F1 Score: 0.9973\n",
      "Epoch 56/80 - Loss: 0.0058, Accuracy: 0.9989, F1 Score: 0.9978\n",
      "Epoch 57/80 - Loss: 0.0053, Accuracy: 0.9987, F1 Score: 0.9975\n",
      "Epoch 58/80 - Loss: 0.0051, Accuracy: 0.9985, F1 Score: 0.9971\n",
      "Epoch 59/80 - Loss: 0.0053, Accuracy: 0.9989, F1 Score: 0.9979\n",
      "Epoch 60/80 - Loss: 0.0049, Accuracy: 0.9985, F1 Score: 0.9972\n",
      "Epoch 61/80 - Loss: 0.0053, Accuracy: 0.9988, F1 Score: 0.9977\n",
      "Epoch 62/80 - Loss: 0.0048, Accuracy: 0.9989, F1 Score: 0.9979\n",
      "Epoch 63/80 - Loss: 0.0045, Accuracy: 0.9986, F1 Score: 0.9973\n",
      "Epoch 64/80 - Loss: 0.0048, Accuracy: 0.9990, F1 Score: 0.9980\n",
      "Epoch 65/80 - Loss: 0.0044, Accuracy: 0.9988, F1 Score: 0.9977\n",
      "Epoch 66/80 - Loss: 0.0044, Accuracy: 0.9986, F1 Score: 0.9974\n",
      "Epoch 67/80 - Loss: 0.0042, Accuracy: 0.9988, F1 Score: 0.9977\n",
      "Epoch 68/80 - Loss: 0.0046, Accuracy: 0.9989, F1 Score: 0.9978\n",
      "Epoch 69/80 - Loss: 0.0044, Accuracy: 0.9990, F1 Score: 0.9982\n",
      "Epoch 70/80 - Loss: 0.0044, Accuracy: 0.9991, F1 Score: 0.9983\n",
      "Epoch 71/80 - Loss: 0.0040, Accuracy: 0.9992, F1 Score: 0.9984\n",
      "Epoch 72/80 - Loss: 0.0041, Accuracy: 0.9990, F1 Score: 0.9982\n",
      "Epoch 73/80 - Loss: 0.0036, Accuracy: 0.9991, F1 Score: 0.9983\n",
      "Epoch 74/80 - Loss: 0.0042, Accuracy: 0.9992, F1 Score: 0.9984\n",
      "Epoch 75/80 - Loss: 0.0039, Accuracy: 0.9991, F1 Score: 0.9983\n",
      "Epoch 76/80 - Loss: 0.0042, Accuracy: 0.9992, F1 Score: 0.9985\n",
      "Epoch 77/80 - Loss: 0.0036, Accuracy: 0.9989, F1 Score: 0.9979\n",
      "Epoch 78/80 - Loss: 0.0039, Accuracy: 0.9993, F1 Score: 0.9987\n",
      "Epoch 79/80 - Loss: 0.0039, Accuracy: 0.9991, F1 Score: 0.9982\n",
      "Epoch 80/80 - Loss: 0.0035, Accuracy: 0.9989, F1 Score: 0.9979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 80\n",
    "\n",
    "\n",
    "# # Calculate class weights\n",
    "class_weights = {0: 1.0, 1: 1.0} \n",
    "weights = torch.tensor([class_weights[1]], dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "model = Net(side_size=14).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_data = TensorDataset(x_train.to(device), y_train.float().unsqueeze(1).to(device))  # Ensure y_train is float and of shape (batch_size, 1)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(x_test.to(device), y_test.float().unsqueeze(1).to(device))  # Ensure y_test is float and of shape (batch_size, 1)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "epoch_losses = []\n",
    "epoch_accuracies = []\n",
    "epoch_f1s = []\n",
    "\n",
    "\n",
    "# # Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)  # Move data to the device\n",
    "        optimizer.zero_grad()\n",
    "        output = torch.sigmoid(model(data))\n",
    "        \n",
    "        # negate the output to match the class weights\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Compute average loss\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "\n",
    "    # Evaluate model and store metrics\n",
    "    train_accuracy, train_f1, _, _ = compute_metrics(train_loader, model)\n",
    "    epoch_accuracies.append(train_accuracy)\n",
    "    epoch_f1s.append(train_f1)\n",
    "\n",
    "    # Enhanced logging\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.4f}, F1 Score: {train_f1:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9dc28",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d75acc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model \n",
    "torch.save(model.state_dict(), './models/malware_deep.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59898f79",
   "metadata": {},
   "source": [
    "# Testing trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a03016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model created\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/phishing_cnn.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m test_accuracy, test_f1, predictions, true_labels \u001b[38;5;241m=\u001b[39m compute_metrics(\u001b[43mtest_loader\u001b[49m, model)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Plotting the confusion matrix\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# load model\n",
    "model = Net(feature_size=173).to(device)\n",
    "model.load_state_dict(torch.load('./models/malware_deep.pth'))\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy, test_f1, predictions, true_labels = compute_metrics(test_loader, model)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix - Test Data')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot for Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epoch_losses, linestyle='--', marker='o', color='#2ba7fc', label=f'Loss (Best: {min(epoch_losses):.4f})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epoch_accuracies, linestyle='--', marker='o', color='#61d484', label=f'Accuracy (Best: {max(epoch_accuracies):.4f})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for F1 Score\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epoch_f1s, linestyle='--', marker='o', color='#b85e4f', label=f'F1 Score (Best: {max(epoch_f1s):.4f})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Training F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle('Training Progress')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the testing results\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "metrics = ['Accuracy', 'F1 Score']\n",
    "values = [test_accuracy, test_f1]\n",
    "colors = ['#61d484', '#b85e4f']\n",
    "\n",
    "plt.bar(metrics, values, color=colors)\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v + 0.02, f\"{v:.4f}\", ha='center', va='bottom')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Test Metrics')\n",
    "plt.show()\n",
    "\n",
    "# bylo 2k na 2k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516b2c0-3ce9-44fc-a8b6-e4c34c420283",
   "metadata": {},
   "source": [
    "# Validate the model on a separate Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c1ae5b-6cc1-4362-ba83-6f4c2491c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2480, 173]) torch.Size([2480])\n",
      "CNN model created\n",
      "torch.Size([2480, 1, 14, 14])\n",
      "torch.Size([2480])\n",
      "tensor([[-37.2285],\n",
      "        [-31.6005],\n",
      "        [-17.6195],\n",
      "        [ 72.9909],\n",
      "        [-19.9294],\n",
      "        [-45.5747],\n",
      "        [-28.6610],\n",
      "        [-10.1254],\n",
      "        [-12.3164],\n",
      "        [-28.5029],\n",
      "        [ -8.3394],\n",
      "        [ -8.2844],\n",
      "        [ 90.8862],\n",
      "        [-12.4166],\n",
      "        [ -4.5517],\n",
      "        [ 32.2141],\n",
      "        [-17.9661],\n",
      "        [-23.2476],\n",
      "        [-20.8448],\n",
      "        [-20.9034],\n",
      "        [ -6.8471],\n",
      "        [ -6.2636],\n",
      "        [-22.9430],\n",
      "        [ -7.7627],\n",
      "        [-14.1914],\n",
      "        [-28.8391],\n",
      "        [-34.7457],\n",
      "        [ 63.7602],\n",
      "        [-16.3150],\n",
      "        [ -9.0472],\n",
      "        [-22.5990],\n",
      "        [-40.9092],\n",
      "        [-20.7916],\n",
      "        [-20.5773],\n",
      "        [-21.7763],\n",
      "        [ -6.0011],\n",
      "        [-24.1876],\n",
      "        [-18.3626],\n",
      "        [-10.9717],\n",
      "        [ -9.7090],\n",
      "        [-16.1542],\n",
      "        [-16.2990],\n",
      "        [-39.4854],\n",
      "        [-11.4905],\n",
      "        [-87.7928],\n",
      "        [-17.5963],\n",
      "        [-10.9451],\n",
      "        [ -3.1364],\n",
      "        [ -8.0356],\n",
      "        [ -5.7000],\n",
      "        [-19.7861],\n",
      "        [-22.4795],\n",
      "        [-33.7179],\n",
      "        [-13.6168],\n",
      "        [-21.9925],\n",
      "        [ -8.7186],\n",
      "        [ -6.7997],\n",
      "        [ -9.0867],\n",
      "        [-62.6797],\n",
      "        [-26.6468],\n",
      "        [-33.9655],\n",
      "        [-11.0813],\n",
      "        [-17.8486],\n",
      "        [-37.6282]], device='cuda:0')\n",
      "tensor([[-28.4988],\n",
      "        [ 33.4597],\n",
      "        [-24.4206],\n",
      "        [-26.5009],\n",
      "        [-23.7777],\n",
      "        [-36.1085],\n",
      "        [-16.5356],\n",
      "        [-24.9414],\n",
      "        [-16.4813],\n",
      "        [ -6.0264],\n",
      "        [-18.5926],\n",
      "        [-37.2236],\n",
      "        [-14.9697],\n",
      "        [-32.3464],\n",
      "        [-20.8318],\n",
      "        [  7.4321],\n",
      "        [-10.8932],\n",
      "        [-25.2496],\n",
      "        [-17.9756],\n",
      "        [ -5.8113],\n",
      "        [ 39.1261],\n",
      "        [ 17.0777],\n",
      "        [-19.9831],\n",
      "        [-34.5929],\n",
      "        [-38.0279],\n",
      "        [-16.3553],\n",
      "        [-25.2070],\n",
      "        [-24.3350],\n",
      "        [-27.2029],\n",
      "        [-14.5203],\n",
      "        [-17.6980],\n",
      "        [-18.3496],\n",
      "        [-11.4620],\n",
      "        [-14.3142],\n",
      "        [ -5.3211],\n",
      "        [-28.6241],\n",
      "        [-29.0878],\n",
      "        [ -7.6887],\n",
      "        [-21.4835],\n",
      "        [-24.8823],\n",
      "        [-14.4140],\n",
      "        [ -8.9579],\n",
      "        [-13.6815],\n",
      "        [-57.6242],\n",
      "        [-21.2437],\n",
      "        [-14.2684],\n",
      "        [-13.2394],\n",
      "        [-50.2064],\n",
      "        [-32.3190],\n",
      "        [-12.9044],\n",
      "        [-20.7840],\n",
      "        [-12.2556],\n",
      "        [-22.6135],\n",
      "        [ -8.1363],\n",
      "        [-10.1038],\n",
      "        [-21.0662],\n",
      "        [ 12.9284],\n",
      "        [-12.0031],\n",
      "        [-17.8014],\n",
      "        [-44.6117],\n",
      "        [-22.8815],\n",
      "        [-12.0318],\n",
      "        [ -3.6593],\n",
      "        [ -9.7531]], device='cuda:0')\n",
      "tensor([[-34.6148],\n",
      "        [-26.1010],\n",
      "        [-13.3534],\n",
      "        [-16.1801],\n",
      "        [-19.3287],\n",
      "        [-42.0261],\n",
      "        [-45.2393],\n",
      "        [-30.8346],\n",
      "        [-28.8211],\n",
      "        [-15.3017],\n",
      "        [-27.2342],\n",
      "        [-23.1098],\n",
      "        [-19.7231],\n",
      "        [-14.7439],\n",
      "        [-17.7985],\n",
      "        [-12.1109],\n",
      "        [-11.0462],\n",
      "        [-16.5813],\n",
      "        [ -9.4972],\n",
      "        [-14.8585],\n",
      "        [-30.1417],\n",
      "        [ 16.3386],\n",
      "        [-25.6679],\n",
      "        [ -3.0160],\n",
      "        [-11.0317],\n",
      "        [ 34.1342],\n",
      "        [-29.6814],\n",
      "        [ -5.7083],\n",
      "        [-30.3868],\n",
      "        [-11.6565],\n",
      "        [ 25.4604],\n",
      "        [-11.2117],\n",
      "        [-17.5945],\n",
      "        [-16.1550],\n",
      "        [-13.8170],\n",
      "        [-12.8656],\n",
      "        [-13.3080],\n",
      "        [-20.3711],\n",
      "        [ -7.7282],\n",
      "        [-21.0096],\n",
      "        [ 40.7989],\n",
      "        [ -9.2035],\n",
      "        [-13.8633],\n",
      "        [-18.7480],\n",
      "        [ -5.7654],\n",
      "        [-20.1151],\n",
      "        [-10.2941],\n",
      "        [-23.9086],\n",
      "        [-22.3371],\n",
      "        [ -5.9142],\n",
      "        [-18.0103],\n",
      "        [-26.8552],\n",
      "        [-15.1414],\n",
      "        [ -9.2901],\n",
      "        [-23.3116],\n",
      "        [-19.6402],\n",
      "        [-19.6878],\n",
      "        [-25.9711],\n",
      "        [-27.4033],\n",
      "        [ -9.4415],\n",
      "        [-13.3086],\n",
      "        [ 31.2895],\n",
      "        [ -8.2419],\n",
      "        [-33.9892]], device='cuda:0')\n",
      "tensor([[ -9.5844],\n",
      "        [-22.1061],\n",
      "        [ -6.5419],\n",
      "        [-44.1077],\n",
      "        [-23.7630],\n",
      "        [ -8.4402],\n",
      "        [ -6.1275],\n",
      "        [-36.8775],\n",
      "        [-14.8586],\n",
      "        [-12.0260],\n",
      "        [-25.7726],\n",
      "        [-19.6366],\n",
      "        [ -7.1316],\n",
      "        [-11.3922],\n",
      "        [-19.8919],\n",
      "        [-25.9298],\n",
      "        [-27.7602],\n",
      "        [-30.2377],\n",
      "        [ -9.3566],\n",
      "        [-46.5807],\n",
      "        [-19.5001],\n",
      "        [-10.4009],\n",
      "        [ -9.3263],\n",
      "        [-18.6654],\n",
      "        [-13.6254],\n",
      "        [ -3.9368],\n",
      "        [-34.6947],\n",
      "        [ -3.4232],\n",
      "        [-24.1073],\n",
      "        [-12.6188],\n",
      "        [ -9.2977],\n",
      "        [-15.6610],\n",
      "        [-12.9312],\n",
      "        [-18.2770],\n",
      "        [ 18.8651],\n",
      "        [ -5.4340],\n",
      "        [ -9.7858],\n",
      "        [-67.7434],\n",
      "        [-19.4125],\n",
      "        [-12.9136],\n",
      "        [-21.2043],\n",
      "        [-13.9556],\n",
      "        [ 96.2925],\n",
      "        [ 47.3726],\n",
      "        [ 54.7519],\n",
      "        [-34.0048],\n",
      "        [-10.2642],\n",
      "        [ 17.5583],\n",
      "        [ 76.0288],\n",
      "        [-14.8293],\n",
      "        [-21.3110],\n",
      "        [-14.9003],\n",
      "        [-32.9095],\n",
      "        [-24.8351],\n",
      "        [-15.1690],\n",
      "        [-10.1200],\n",
      "        [ 38.2211],\n",
      "        [ -9.0126],\n",
      "        [-40.7728],\n",
      "        [ -7.1924],\n",
      "        [-13.0744],\n",
      "        [ 53.9269],\n",
      "        [-26.9037],\n",
      "        [-54.6915]], device='cuda:0')\n",
      "tensor([[-29.4515],\n",
      "        [ -8.1093],\n",
      "        [-18.3308],\n",
      "        [-17.2245],\n",
      "        [-20.6926],\n",
      "        [-14.3124],\n",
      "        [-43.5312],\n",
      "        [-29.8001],\n",
      "        [ -6.4393],\n",
      "        [ 89.2731],\n",
      "        [-16.3624],\n",
      "        [-11.2004],\n",
      "        [-13.8562],\n",
      "        [-17.7523],\n",
      "        [-18.6781],\n",
      "        [ -6.2770],\n",
      "        [-26.6875],\n",
      "        [ 65.7901],\n",
      "        [-14.1279],\n",
      "        [-34.2835],\n",
      "        [-38.2995],\n",
      "        [-28.2847],\n",
      "        [-13.1587],\n",
      "        [-12.8535],\n",
      "        [ 10.0765],\n",
      "        [-23.5977],\n",
      "        [-34.4082],\n",
      "        [-20.3266],\n",
      "        [ 12.6768],\n",
      "        [-26.6284],\n",
      "        [-19.8134],\n",
      "        [-10.8329],\n",
      "        [-21.2260],\n",
      "        [ -7.0549],\n",
      "        [ 78.0237],\n",
      "        [-19.6447],\n",
      "        [ 64.6304],\n",
      "        [-33.3984],\n",
      "        [-16.6727],\n",
      "        [-26.0448],\n",
      "        [107.1400],\n",
      "        [-14.2046],\n",
      "        [-34.3330],\n",
      "        [-12.3866],\n",
      "        [-15.0300],\n",
      "        [-11.2645],\n",
      "        [-38.2648],\n",
      "        [-31.2515],\n",
      "        [-28.0465],\n",
      "        [ -9.5242],\n",
      "        [-20.3573],\n",
      "        [ -8.6367],\n",
      "        [-12.2263],\n",
      "        [ -8.7557],\n",
      "        [ -9.8700],\n",
      "        [-17.7533],\n",
      "        [-27.2388],\n",
      "        [ -9.8596],\n",
      "        [-19.4117],\n",
      "        [-10.3649],\n",
      "        [ -7.7555],\n",
      "        [ -9.0582],\n",
      "        [-27.4036],\n",
      "        [-41.9305]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from pandas.core.dtypes import common as com\n",
    "import numpy as np\n",
    "\n",
    "def cast_timestamp(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Cast timestamp fields to seconds since epoch.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if com.is_timedelta64_dtype(df[col]):\n",
    "            df[col] = df[col].dt.total_seconds()  # This converts timedelta to float (seconds)\n",
    "        elif com.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = df[col].astype(np.int64) // 10**9  # Converts datetime64 to Unix timestamp (seconds)\n",
    "    return df\n",
    "\n",
    "# Load the validation dataset\n",
    "validation_dataset_filename = '../testdata/validation_malware.parquet'\n",
    "df_validation = pq.read_table(validation_dataset_filename).to_pandas()\n",
    "\n",
    "# Cast timestamps and handle NaNs\n",
    "df_validation = cast_timestamp(df_validation)\n",
    "df_validation.fillna(-1, inplace=True)\n",
    "\n",
    "# Remove disqualified columns\n",
    "for column in disqualified_columns:\n",
    "    if column in df_validation.columns:\n",
    "        df_validation.drop(column, axis=1, inplace=True)\n",
    "\n",
    "# Map the labels\n",
    "df_validation['label'] = df_validation['label'].map({'benign': 0, 'phishing': 1})\n",
    "\n",
    "# print number of columns\n",
    "\n",
    "\n",
    "# Extract features and labels\n",
    "X_val = df_validation.drop(['label', 'domain_name'], axis=1)\n",
    "y_val = df_validation['label']\n",
    "\n",
    "# Load the scaler\n",
    "scaler = joblib.load(\"boundaries/malware_general_scaler.joblib\")\n",
    "\n",
    "# Scale the features\n",
    "y_val = y_val.tolist()\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "# convert to tensor\n",
    "X_val_scaled = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "# convert pandas series y_val to llist\n",
    "\n",
    "# print shape of X_val_scaled and y_val_tensor\n",
    "\n",
    "# print number of columns\n",
    "\n",
    "# print shape of X_val_scaled and y_val_tensor\n",
    "print(X_val_scaled.shape, y_val_tensor.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "model = Net(feature_size=173)\n",
    "model.load_state_dict(torch.load('./models/malware_deep.pth'))\n",
    "model.eval()\n",
    "model.to('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#### PADDING ###\n",
    "# Helper function to find the next perfect square\n",
    "\n",
    "\n",
    "\n",
    "pickle.dump(X_val_scaled, open('old_malware_sequence_x.pkl', 'wb'))\n",
    "pickle.dump(y_val_tensor, open('old_malware_sequence_y.pkl', 'wb'))\n",
    "\n",
    "\n",
    "val_data = TensorDataset(X_val_scaled, y_val_tensor)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Function to compute metrics and predictions\n",
    "def compute_metrics_and_predictions(data_loader, model):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            print(torch.sigmoid(output).round())\n",
    "            print(torch.sigmoid(output))\n",
    "            input()\n",
    "            true_labels.extend(target.cpu().numpy())\n",
    "            predictions.extend(torch.sigmoid(output).round().cpu().numpy())\n",
    "\n",
    "\n",
    "    return true_labels, predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get predictions and probabilities\n",
    "test_true_labels, test_predictions= compute_metrics_and_predictions(val_loader, model)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "weighted_accuracy = (accuracy_score(np.array(test_true_labels) == 0, np.array(test_predictions) == 0) + accuracy_score(np.array(test_true_labels) == 1, np.array(test_predictions) == 1)) / 2\n",
    "precision = precision_score(test_true_labels, test_predictions)\n",
    "recall = recall_score(test_true_labels, test_predictions)\n",
    "f1 = f1_score(test_true_labels, test_predictions)\n",
    "tn, fp, fn, tp = confusion_matrix(test_true_labels, test_predictions).ravel()\n",
    "false_positive_rate = fp / (fp + tn)\n",
    "\n",
    "# Display metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Weighted Accuracy: {weighted_accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'False Positive Rate: {false_positive_rate}')\n",
    "\n",
    "# Display confusion matrix\n",
    "cm = confusion_matrix(test_true_labels, test_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['benign', 'phishing'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix - Test Data')\n",
    "plt.show()\n",
    "\n",
    "# Identify misclassified domain names\n",
    "misclassified = df_validation.iloc[[i for i, (y_true, y_pred) in enumerate(zip(test_true_labels, test_predictions)) if y_true != y_pred]]\n",
    "misclassified_domains = misclassified['domain_name'].tolist()\n",
    "print(f'Misclassified domains: {misclassified_domains}')\n",
    "\n",
    "# bylo x 1000\n",
    "# 10 x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca2912-6e1c-4ecf-a4eb-274882896e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "472a3593",
   "metadata": {},
   "source": [
    "# Make test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "11ec9657",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at dga_binary_model.keras",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [218], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Table\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the model and scaler\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdga_binary_model.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdga_binary_scaler.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# #############################################################\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# EDIT this to specify benign / malicious datasets to use     #\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# #############################################################\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/legacy/save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    240\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    241\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at dga_binary_model.keras"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from pyarrow import Table\n",
    "\n",
    "# Load the model and scaler\n",
    "model = load_model('dga_binary_model.keras')\n",
    "scaler = joblib.load(\"dga_binary_scaler.joblib\")\n",
    "\n",
    "# #############################################################\n",
    "# EDIT this to specify benign / malicious datasets to use     #\n",
    "# #############################################################\n",
    "benign_dataset_filenames = [\n",
    "    '../feature-extraction/floor/benign_2312_anonymized.parquet',\n",
    "    '../feature-extraction/floor/umbrella_benign_FINISHED.parquet',\n",
    "]\n",
    "malicious_dataset_filenames = [\n",
    "    '../feature-extraction/floor/lex-dga-830k-pick.parquet'\n",
    "]\n",
    "# #############################################################\n",
    "# EDIT this for to set appropriate labels (malware, dga, ...) #\n",
    "# #############################################################\n",
    "benign_label = \"benign\"\n",
    "malicious_label = \"dga\"\n",
    "# #############################################################\n",
    "\n",
    "def union_tables(tables: [pa.Table]) -> pa.Table:\n",
    "    union_table = tables[0]\n",
    "    for table in tables[1:]:\n",
    "        right_not_in_union = union_table.join(right_table=table, keys='domain_name', join_type='right anti',\n",
    "                                              coalesce_keys=True, use_threads=True)\n",
    "        union_table = pa.concat_tables([union_table, right_not_in_union])\n",
    "    return union_table\n",
    "\n",
    "# Unify malicious datasets and benign datasets\n",
    "schema = (pq.read_table(malicious_dataset_filenames[0])).schema # Use the schema from the first malicious filename\n",
    "benign_tables = [pq.read_table(filename).cast(schema) for filename in benign_dataset_filenames]\n",
    "malicious_tables = [pq.read_table(filename).cast(schema) for filename in malicious_dataset_filenames]\n",
    "malicious = union_tables(malicious_tables)\n",
    "benign = union_tables(benign_tables)\n",
    "\n",
    "# Convert pyarrow tables to pandas dataframes\n",
    "df_benign = benign.to_pandas()\n",
    "df_malicious = malicious.to_pandas()\n",
    "\n",
    "# Set appropriate labels\n",
    "df_benign[\"label\"] = benign_label\n",
    "df_malicious[\"label\"] = malicious_label\n",
    "class_map = {benign_label: 0, malicious_label: 1}\n",
    "\n",
    "# Concatentate benign and malicious\n",
    "test_df = pd.concat([df_benign, df_malicious])\n",
    "\n",
    "# Handle NaNs\n",
    "test_df.fillna(-1, inplace=True)\n",
    "\n",
    "\n",
    "# Take only N random samples\n",
    "N = 500\n",
    "test_df = test_df.sample(n=N, random_state=42)\n",
    "\n",
    "total_predictions = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    domain_name = row['domain_name']\n",
    "    original_label = row['label']\n",
    "    \n",
    "    # Drop \"domain_name\" and \"label\" columns\n",
    "    feature_vector = pd.DataFrame([row])\n",
    "    feature_vector.drop(columns=['domain_name', 'label'], inplace=True)\n",
    "\n",
    "     # Scale the feature vector using the loaded scaler\n",
    "    scaled_feature_vector = scaler.transform(feature_vector)\n",
    "    \n",
    "    # Perform prediction\n",
    "    prediction = model.predict(scaled_feature_vector, verbose=0)\n",
    "    \n",
    "    # Extract the predicted class\n",
    "    predicted_label = \"benign\" if prediction < 0.5 else \"dga\"\n",
    "    \n",
    "    # Check if the prediction was correct\n",
    "    if original_label == predicted_label:\n",
    "        correct_predictions += 1\n",
    "    \n",
    "    total_predictions += 1\n",
    "    \n",
    "    # Print the result\n",
    "    result=\"WRONG\"\n",
    "    if predicted_label == original_label:\n",
    "        result=\"OK\"\n",
    "        \n",
    "    pred_disp = \"!!! DGA !!!\"\n",
    "    if predicted_label == \"benign\":\n",
    "        pred_disp = \"BENIGN\"\n",
    "        \n",
    "    \n",
    "    print(f\"{result} | {domain_name} ({original_label}), Predicted: {pred_disp}, Prob: {prediction}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3d6c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e63ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206433e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47d300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
